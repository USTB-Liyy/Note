# 论文阅读

## LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS

背景：大模型参数微调困难，现有技术通过扩展模型深度或减少模型的可用序列长度，在模型质量和效率间做平衡

## Attention Is All You Need

2017 NIPS

1. Multi-Head Attention
2. Scaled Dot-Product Attention
3. Layer-Normalization
4. residual connect
5. Encoder and decoder stacks
6. Position-wise Feed-Forward Networks
7. Positional Encoding
8. Masked Multi-Head Attention

## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

* **B**idirectional **E**ncoder **R**epresentations from **T**ransformers: 多个Transformer的编码器堆叠，深层Transformer

* Two kinds of tasks:
  1. **sentence-level tasks**: natural language inference, paraphrasing
  2. **token-level tasks**: named entity recognition, question answering

* pre-training approaches:
  1. Unsupervised Feature-based Approaches
  2. Unsupervised Fine-tuning Approaches
  3. Transfer Learning from Supervised Data(Computer vision)

* two phases of BERT: pre-training and fine-tuning

* BERT’s **model architecture** is a multi-layer bidirectional Transformer en- coder based on the original implementation

* Masked LM, Next Sentence Prediction


## Improving Language Understanding by Generative Pre-Training

**G**enerative **P**re-**T**raining：

* two stages：1. learning a high-capacity language model on a large corpus of text.(**Unsupervised pre-training**) 2. adapt the model to a discriminative task with labeled data.(**Supervised fine-tuning**)

* Unsupervised pre-training:

  * 最大化似然函数（已有前k个token和模型参数预测下一个token）训练模型时的损失函数
    $L_{1}(\mathcal{U})=\sum_{i} \log P\left(u_{i} \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)$
  * $h_0=UW_e+W_p$
    $h_l=transformer_block(h_{l-1})\forall i \in[1, n]$
    $P(u)=softmax(h_nW_e^T)$

* Supervised fine-tuning：

  * 微调根据打标签的数据最大化目标函数：

    $P\left(y \mid x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)$
    $L_{2}(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^{1}, \ldots, x^{m}\right)$

## GLM: General Language Model Pretraining with Autoregressive Blank Infilling

ChatGLM

**autoregressive**, **autoencoding**, and **encoder-decoder models**
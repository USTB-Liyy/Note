# 大模型

## LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS

背景：大模型参数微调困难，现有技术通过扩展模型深度或减少模型的可用序列长度，在模型质量和效率间做平衡

## Attention Is All You Need

2017 NIPS

1. Multi-Head Attention
2. Scaled Dot-Product Attention
3. Layer-Normalization
4. residual connect
5. Encoder and decoder stacks
6. Position-wise Feed-Forward Networks
7. Positional Encoding
8. Masked Multi-Head Attention

## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

* **B**idirectional **E**ncoder **R**epresentations from **T**ransformers: 多个Transformer的编码器堆叠，深层Transformer

* Two kinds of tasks:
  1. **sentence-level tasks**: natural language inference, paraphrasing
  2. **token-level tasks**: named entity recognition, question answering

* pre-training approaches:
  1. Unsupervised Feature-based Approaches
  2. Unsupervised Fine-tuning Approaches
  3. Transfer Learning from Supervised Data(Computer vision)

* two phases of BERT: pre-training and fine-tuning

* BERT’s **model architecture** is a multi-layer bidirectional Transformer en- coder based on the original implementation

* Masked LM, Next Sentence Prediction


## Improving Language Understanding by Generative Pre-Training

**G**enerative **P**re-**T**raining：

* two stages：1. learning a high-capacity language model on a large corpus of text.(**Unsupervised pre-training**) 2. adapt the model to a discriminative task with labeled data.(**Supervised fine-tuning**)

* Unsupervised pre-training:

  * 最大化似然函数（已有前k个token和模型参数预测下一个token）训练模型时的损失函数
    $L_{1}(\mathcal{U})=\sum_{i} \log P\left(u_{i} \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)$
  * $h_0=UW_e+W_p$
    $h_l=transformer_block(h_{l-1})\forall i \in[1, n]$
    $P(u)=softmax(h_nW_e^T)$

* Supervised fine-tuning：

  * 微调根据打标签的数据最大化目标函数：

    $P\left(y \mid x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)$
    $L_{2}(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^{1}, \ldots, x^{m}\right)$

# 微调

## Prefix-Tuning：Optimizing Continuous Prompts for Generation

**Prefix-Tuning**

* 是后续读的几个论文的微调方法中（Prefix-Tuning、P-Tuning、Prompt-Tuning），最优先提出的。在Prefix-Tuning提出前，微调还是要修改语言模型所有的参数，并且针对每一个任务需要存储复制全部的参数。学习的参数少（0.1%）能达到和全数据微调（full data setting）想近似效果，并远优于少数据微调（low-data settings），在推断未在训练集中出现的样例时有更好的效果。先前的是使用adapter-tuning，预训练大模型每层加入adapter，微调大约3.6%的大模型参数。
* 微调包含两种：
  1. BERT+finetuning
  2. 对大模型进行微调ChatGLM

* NLP 任务的发展分为四个阶段即 NLP 四范式：

  1. 第一范式：基于传统机器学习模型的范式，如 tf-idf 特征 + 朴素贝叶斯等机器算法；

  2. 第二范式：基于深度学习模型的范式，如 word2vec 特征 + LSTM 等深度学习算法，相比于第一范式，模型准确有所提高，特征工程的工作也有所减少；

  3. 第三范式：基于预训练模型 + finetuning 的范式，如 BERT + finetuning 的 NLP 任务，相比于第二范式，模型准确度显著提高，但是模型也随之变得更大，但小数据集就可训练出好模型；

  4. 第四范式：基于预训练模型 + Prompt + 预测的范式，如 BERT + Prompt 的范式相比于第三范式，模型训练所需的训练数据显著减少。
     * prompt-learning和fine-tuning两种思路。fine-tuning是针对下游任务，使用特定数据集对模型进行修改。prompt-learning在尽量少地改变模型的情况下，通过给输入提供“提示信息”，将下游任务转化为完形填空或者文本生成任务，充分挖掘预训练模型学到的知识。

* 

# 读论文范式

问题、方法、思路、优势、缺陷、表达


* https://github.com/THUDM/ChatGLM2-6B

    >   1. chatglm2可以流式生成文本
    >
    >   2. 有简单的网页实现
    >
    >   3. 有多卡部署的解决方案
    >
    >       如果你有多张 GPU，但是每张 GPU 的显存大小都不足以容纳完整的模型，那么可以将模型切分在多张GPU上。首先安装 accelerate: `pip install accelerate`，然后通过如下方法加载模型：
    >
    >       ```
    >       from utils import load_model_on_gpus
    >       model = load_model_on_gpus("THUDM/chatglm2-6b", num_gpus=2)
    >       ```
    >
    >       即可将模型部署到两张 GPU 上进行推理。你可以将 `num_gpus` 改为你希望使用的 GPU 数。默认是均匀切分的，你也可以传入 `device_map` 参数来自己指定。
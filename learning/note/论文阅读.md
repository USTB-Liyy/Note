# 论文阅读

## LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS

背景：大模型参数微调困难，现有技术通过扩展模型深度或减少模型的可用序列长度，在模型质量和效率间做平衡

## Attention Is All You Need

2017 NIPS

1. Multi-Head Attention
2. Scaled Dot-Product Attention
3. Layer-Normalization
4. residual connect
5. Encoder and decoder stacks
6. Position-wise Feed-Forward Networks
7. Positional Encoding
8. Masked Multi-Head Attention

## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

* **B**idirectional **E**ncoder **R**epresentations from **T**ransformers: 多个Transformer的编码器堆叠，深层Transformer

* Two kinds of tasks:
  1. **sentence-level tasks**: natural language inference, paraphrasing
  2. **token-level tasks**: named entity recognition, question answering

* pre-training approaches:
  1. Unsupervised Feature-based Approaches
  2. Unsupervised Fine-tuning Approaches
  3. Transfer Learning from Supervised Data(Computer vision)

* two phases of BERT: pre-training and fine-tuning

* BERT’s **model architecture** is a multi-layer bidirectional Transformer en- coder based on the original implementation

* Masked LM, Next Sentence Prediction


## Improving Language Understanding by Generative Pre-Training

**G**enerative **P**re-**T**raining

## GLM: General Language Model Pretraining with Autoregressive Blank Infilling

ChatGLM
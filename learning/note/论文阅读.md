# 论文阅读

## LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS

背景：大模型参数微调困难，现有技术通过扩展模型深度或减少模型的可用序列长度，在模型质量和效率间做平衡

## Attention Is All You Need

2017 NIPS

1. Multi-Head Attention
2. Scaled Dot-Product Attention
3. Layer-Normalization
4. residual connect
5. Encoder and decoder stacks
6. Position-wise Feed-Forward Networks
7. Positional Encoding
8. Masked Multi-Head Attention

## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

**B**idirectional **E**ncoder **R**epresentations from **T**ransformers

Autoregressive models: GPT
Autoencoding models: BERT

BERT: pre-training and fine-tuning

During **pre-training**, the model is trained on **unlabeled data** over different pre-training tasks. For **fine-tuning**, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using **labeled data** from the downstream tasks.

## GLM: General Language Model Pretraining with Autoregressive Blank Infilling

ChatGLM